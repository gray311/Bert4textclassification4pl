{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/.conda/envs/python37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Disease', 1: 'Symptom', 2: 'Sign', 3: 'Pregnancy-related Activity', 4: 'Neoplasm Status', 5: 'Non-Neoplasm Disease Stage', 6: 'Allergy Intolerance', 7: 'Organ or Tissue Status', 8: 'Life Expectancy', 9: 'Oral related', 10: 'Pharmaceutical Substance or Drug', 11: 'Therapy or Surgery', 12: 'Device', 13: 'Nursing', 14: 'Diagnostic', 15: 'Laboratory Examinations', 16: 'Risk Assessment', 17: 'Receptor Status', 18: 'Age', 19: 'Special Patient Characteristic', 20: 'Literacy', 21: 'Gender', 22: 'Education', 23: 'Address', 24: 'Ethnicity', 25: 'Consent', 26: 'Enrollment in other studies', 27: 'Researcher Decision', 28: 'Capacity', 29: 'Ethical Audit', 30: 'Compliance with Protocol', 31: 'Addictive Behavior', 32: 'Bedtime', 33: 'Exercise', 34: 'Diet', 35: 'Alcohol Consumer', 36: 'Sexual related', 37: 'Smoking Status', 38: 'Blood Donation', 39: 'Encounter', 40: 'Disabilities', 41: 'Healthy', 42: 'Data Accessible', 43: 'Multiple'}\n",
      "22962\n",
      "7682\n",
      "10192\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'label', 'text', 'labels'],\n",
      "        num_rows: 22962\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'label', 'text', 'labels'],\n",
      "        num_rows: 7682\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 10192\n",
      "    })\n",
      "})\n",
      "{'id': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None)}\n",
      "{'id': 's1', 'label': 'Therapy or Surgery', 'text': ' 研究开始前30天内，接受过其他临床方案治疗；', 'labels': 11}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seedeverything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  ##\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seedeverything(seed=233)\n",
    "\n",
    "traindata = pd.read_json('./CHIP-CTC/CHIP-CTC_train.json')\n",
    "valdata = pd.read_json('./CHIP-CTC/CHIP-CTC_dev.json')\n",
    "testdata = pd.read_json('./CHIP-CTC/CHIP-CTC_test.json')\n",
    "\n",
    "examplepreddata = pd.read_excel('./CHIP-CTC/category.xlsx')\n",
    "\n",
    "examplepreddata['label2idx'] = range(examplepreddata.shape[0])\n",
    "\n",
    "label2idx = dict(\n",
    "    zip(examplepreddata['Label Name'], examplepreddata['label2idx']))\n",
    "idx2label = dict(\n",
    "    zip(examplepreddata['label2idx'], examplepreddata['Label Name']))\n",
    "\n",
    "print(idx2label)\n",
    "\n",
    "traindata['labels'] = [label2idx[item] for item in traindata['label']]\n",
    "valdata['labels'] = [label2idx[item] for item in valdata['label']]\n",
    "\n",
    "print(len(traindata))\n",
    "print(len(valdata))\n",
    "print(len(testdata))\n",
    "\n",
    "traindataset = Dataset.from_pandas(traindata)\n",
    "valdataset = Dataset.from_pandas(valdata)\n",
    "testdataset = Dataset.from_pandas(testdata)\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': traindataset,\n",
    "    'validation': valdataset,\n",
    "    'test': testdataset\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "print(train_dataset.features)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7f4ffa550710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 23/23 [00:03<00:00,  7.34ba/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  7.84ba/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  8.56ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 22962\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 7682\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 10192\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('uer/chinese_roberta_L-8_H-512',\n",
    "                                          mirror='tuna')\n",
    "\n",
    "def tokenize_function(sample):\n",
    "    return tokenizer(sample['text'], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(\n",
    "    ['id', 'text', 'label'])\n",
    "tokenized_datasets['validation'] = tokenized_datasets[\n",
    "    'validation'].remove_columns(['id', 'text', 'label'])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns(\n",
    "    ['id', 'text'])\n",
    "\n",
    "from transformers import DataCollatorWithPadding  #实现按batch自动padding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([32, 76]), 'token_type_ids': torch.Size([32, 76]), 'attention_mask': torch.Size([32, 76])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=32, collate_fn=data_collator)  \n",
    "val_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=32, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=32, collate_fn=data_collator)\n",
    "for batch in test_dataloader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uer/chinese_roberta_L-8_H-512 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at uer/chinese_roberta_L-8_H-512 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(3.7834, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0997,  0.0356,  0.0691,  ..., -0.0297,  0.0373,  0.1272],\n",
      "        [ 0.0572,  0.0843,  0.0407,  ..., -0.0085,  0.0295,  0.1387],\n",
      "        [ 0.1198,  0.0831,  0.1267,  ..., -0.0696,  0.0629,  0.0540],\n",
      "        ...,\n",
      "        [ 0.0499, -0.0527,  0.0166,  ...,  0.0521,  0.0730,  0.0817],\n",
      "        [ 0.0578,  0.0536,  0.1319,  ..., -0.0502,  0.0895,  0.0580],\n",
      "        [ 0.0762, -0.0394,  0.0129,  ...,  0.0482,  0.0537,  0.0232]],\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'uer/chinese_roberta_L-8_H-512'\n",
    "net = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=examplepreddata.shape[0])\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    outputs = net(**batch)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5744\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import AdamW, get_scheduler\n",
    "from datasets import load_metric\n",
    "from statistics import mean\n",
    "from sklearn import metrics\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 8\n",
    "lr = 3e-5\n",
    "num_training_steps = num_epochs * len(train_dataloader)  # num of batches * num of epochs\n",
    "print(num_training_steps)\n",
    "\n",
    "\n",
    "class Bert4textclassification_lightningsystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,net,lr,epoch,len):\n",
    "        super(Bert4textclassification_lightningsystem, self).__init__()\n",
    "        self.net = net.to(device)\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.num_training_steps = len\n",
    "        #self.metric = load_metric(\"glue\", \"mrpc\",mirror=\"tuna\")\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        self.optimizer = AdamW(self.net.parameters(), lr=self.lr)\n",
    "        lr_scheduler = get_scheduler(\n",
    "                'linear',\n",
    "                optimizer=self.optimizer, \n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=self.num_training_steps)\n",
    "        optim_dict = {'optimizer': self.optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        return optim_dict\n",
    "    def metrics_compute(self,mode,outputs):\n",
    "        loss = []\n",
    "        loss.append(outputs[0][mode+'_loss'])\n",
    "        predictions = outputs[0]['predictions']\n",
    "        labels = outputs[0]['labels']\n",
    "        for i in range(1,len(outputs)):\n",
    "            loss.append(outputs[i][mode+'_loss'])\n",
    "            predictions = torch.concat([predictions,outputs[i]['predictions']],dim=0)\n",
    "            labels = torch.concat([labels,outputs[i]['labels']],dim=0)\n",
    "        loss = torch.tensor(loss)\n",
    "        predictions = predictions.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        return loss,predictions,labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        loss = self.net(**batch).loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metrics_dict = metrics.classification_report(predictions.cpu().detach().numpy(),batch['labels'].cpu().detach().numpy(),digits = 4,output_dict=True)\n",
    "        self.log('val_weighted_f1',metrics_dict['weighted avg']['f1-score'])\n",
    "        #self.metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        return {'val_loss':outputs.loss,'predictions':predictions,'labels':batch['labels']}\n",
    "    \n",
    "       \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        return {'test_loss':outputs.loss,'predictions':predictions}\n",
    "    \n",
    "    def training_epoch_end(self,outputs):\n",
    "        pass\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print(outputs[0]['predictions'].shape)\n",
    "        print(len(outputs))\n",
    "        val_loss ,predictions,labels= self.metrics_compute('val',outputs)\n",
    "        print(predictions.shape)\n",
    "        print('\\n',\"val_loss: \",val_loss.mean())\n",
    "        print(metrics.classification_report(predictions, labels,digits = 4))\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        predictions = outputs[0]['predictions']\n",
    "        for i in range(1,len(outputs)):\n",
    "            predictions = torch.concat([predictions,outputs[i]['predictions']],dim=0)\n",
    "        predictions = predictions.cpu().detach().numpy().tolist()\n",
    "        test_labels =[ idx2label[idx] for idx in predictions]\n",
    "        testdata['label'] = test_labels\n",
    "        test_pred_list = []\n",
    "        for i in range(testdata.shape[0]):\n",
    "            temp_dict = {}\n",
    "            temp_dict['id'] = testdata.iloc[i,0]\n",
    "            temp_dict['label'] = testdata.iloc[i,2]\n",
    "            temp_dict['text'] = testdata.iloc[i,1]\n",
    "            test_pred_list.append(temp_dict)\n",
    "        print('\\n',testdata.head())\n",
    "        with open(\"result.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(test_pred_list, fp, ensure_ascii=False, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2662/2640533919.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBert4textclassification_lightningsystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_training_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m checkpoint_callback = ModelCheckpoint(\n\u001b[1;32m      3\u001b[0m             \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_weighted_f1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mdirpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2662/308549305.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, net, lr, epoch, len)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBert4textclassification_lightningsystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.conda/envs/python37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Bert4textclassification_lightningsystem(net,lr,num_epochs,num_training_steps)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_weighted_f1',\n",
    "            dirpath='./output',\n",
    "            filename=\n",
    "            'chinese_roberta_L-8_H-512-CHIP-CTC-{epoch:02d}-{val_weighted_f1:.3f}',\n",
    "            mode='max')\n",
    "trainer = Trainer(\n",
    "            logger=False,\n",
    "            max_epochs=num_epochs,\n",
    "            gpus=1,\n",
    "            reload_dataloaders_every_n_epochs=False,\n",
    "            num_sanity_val_steps=0,  # Skip Sanity Check\n",
    "            callbacks=[checkpoint_callback],\n",
    "            #limit_train_batches=0.05\n",
    "            #precision=16,\n",
    "            #accumulate_grad_batches=2,\n",
    "            #gradient_clip_val=0.5,\n",
    "        )\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 319/319 [00:04<00:00, 67.77it/s]\n",
      "    id                                     text               label\n",
      "0  s1                         4.手术体位不适合BIS 监测；              Device\n",
      "1  s2                            5.术前未能开放静脉通路。  Therapy or Surgery\n",
      "2  s3  1.年龄18岁至40岁有生育要求的重度宫腔粘连（IV度、Va度及Vb度）患者；            Multiple\n",
      "3  s4             2.生殖系统无器质性病变，术前病理检查无子宫内膜病变者；             Disease\n",
      "4  s5                          3.同意并签署患者知情同意书；             Consent\n",
      "Testing DataLoader 0: 100%|██████████| 319/319 [00:05<00:00, 60.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Bert4textclassification_lightningsystem.load_from_checkpoint(checkpoint_path='./output/chinese_roberta_L-8_H-512-CHIP-CTC-epoch=02-val_weighted_f1=0.843.ckpt',\n",
    "net=net,lr=lr,epoch=num_epochs,len=num_training_steps\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "            logger=False,\n",
    "            gpus=1,\n",
    "            #limit_train_batches=0.05\n",
    "            #precision=16,\n",
    "            #accumulate_grad_batches=2,\n",
    "            #gradient_clip_val=0.5,\n",
    "        )\n",
    "trainer.test(model=model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90b94d728fc700a31971639fb0a12048c9a464edc079be00035098c1bbe95ef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
