{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from nlpcda import Similarword\n",
    "\n",
    "\n",
    "def seedeverything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  ##\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seedeverything(seed=233)\n",
    "\n",
    "traindata = pd.read_json('./CHIP-CTC/CHIP-CTC_train.json')\n",
    "valdata = pd.read_json('./CHIP-CTC/CHIP-CTC_dev.json')\n",
    "testdata = pd.read_json('./CHIP-CTC/CHIP-CTC_test.json')\n",
    "testdata_temp = deepcopy(testdata)\n",
    "\n",
    "\n",
    "def textclean(x):\n",
    "    a = re.findall('[\\u4e00-\\u9fa5A-Za-z][\\S\\s]+',x,re.S)  \n",
    "    a = \"\".join(a)\n",
    "    return a\n",
    "\n",
    "traindata['text'] = traindata['text'].apply(lambda x: textclean(x))\n",
    "valdata['text'] = valdata['text'].apply(lambda x: textclean(x))\n",
    "testdata['text'] = testdata['text'].apply(lambda x: textclean(x))\n",
    "\n",
    "\n",
    "examplepreddata = pd.read_excel('./CHIP-CTC/category.xlsx')\n",
    "\n",
    "examplepreddata['label2idx'] = range(examplepreddata.shape[0])\n",
    "\n",
    "label2idx = dict(\n",
    "    zip(examplepreddata['Label Name'], examplepreddata['label2idx']))\n",
    "idx2label = dict(\n",
    "    zip(examplepreddata['label2idx'], examplepreddata['Label Name']))\n",
    "\n",
    "with open(\"idx2label.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(idx2label, fp, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(idx2label)\n",
    "\n",
    "traindata['labels'] = [label2idx[item] for item in traindata['label']]\n",
    "valdata['labels'] = [label2idx[item] for item in valdata['label']]\n",
    "\n",
    "print(len(traindata))\n",
    "print(len(valdata))\n",
    "print(len(testdata))\n",
    "\n",
    "traindataset = Dataset.from_pandas(traindata)\n",
    "valdataset = Dataset.from_pandas(valdata)\n",
    "testdataset = Dataset.from_pandas(testdata)\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': traindataset,\n",
    "    'validation': valdataset,\n",
    "    'test': testdataset\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "print(train_dataset.features)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"hflchinese-bert-wwm-withpretrain-ext\"\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(sample):\n",
    "    return tokenizer(sample['text'], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(\n",
    "    ['id', 'text', 'label'])\n",
    "tokenized_datasets['validation'] = tokenized_datasets[\n",
    "    'validation'].remove_columns(['id', 'text', 'label'])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns(\n",
    "    ['id', 'text'])\n",
    "\n",
    "from transformers import DataCollatorWithPadding  #实现按batch自动padding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=8, collate_fn=data_collator)  \n",
    "val_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=8, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=8, collate_fn=data_collator)\n",
    "for batch in test_dataloader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification,AutoModel\n",
    "from transformers import BertForSequenceClassification,BertForMaskedLM\n",
    "\n",
    "net = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=examplepreddata.shape[0])\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    outputs = net(**batch)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.8546\n",
    "\n",
    "-1.4115e-01\n",
    "\n",
    "4.1694\n",
    "'''\n",
    "from transformers import AdamW, get_scheduler\n",
    "from datasets import load_metric\n",
    "from statistics import mean\n",
    "from sklearn import metrics\n",
    "from torch import nn\n",
    "import json\n",
    "import warnings\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 5\n",
    "lr = 2.5e-5\n",
    "num_labels = examplepreddata.shape[0] \n",
    "num_training_steps = num_epochs * len(train_dataloader)  # num of batches * num of epochs\n",
    "print(num_training_steps)\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=1000,\n",
    "                 out_features=None,\n",
    "                 act_layer=nn.GELU,\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class Bert4textclassification_lightningsystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,net,lr,epoch,len):\n",
    "        super(Bert4textclassification_lightningsystem, self).__init__()\n",
    "        self.net = net.to(device)\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.num_training_steps = len\n",
    "        self.writer = SummaryWriter('./log-'+checkpoint)\n",
    "        self.iteration = 0\n",
    "        #self.metric = load_metric(\"glue\", \"mrpc\",mirror=\"tuna\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        self.optimizer = AdamW(self.net.parameters(), lr=self.lr)\n",
    "        lr_scheduler = get_scheduler(\n",
    "                'linear',\n",
    "                optimizer=self.optimizer, \n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=self.num_training_steps)\n",
    "        optim_dict = {'optimizer': self.optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        return optim_dict\n",
    "        \n",
    "    def metrics_compute(self,mode,outputs):\n",
    "        loss = []\n",
    "        loss.append(outputs[0][mode+'_loss'])\n",
    "        predictions = outputs[0]['predictions']\n",
    "        labels = outputs[0]['labels']\n",
    "        for i in range(1,len(outputs)):\n",
    "            loss.append(outputs[i][mode+'_loss'])\n",
    "            predictions = torch.concat([predictions,outputs[i]['predictions']],dim=0)\n",
    "            labels = torch.concat([labels,outputs[i]['labels']],dim=0)\n",
    "        loss = torch.tensor(loss)\n",
    "        predictions = predictions.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        return loss,predictions,labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        loss = outputs.loss\n",
    "        lr_ = self.lr * (1.0 -\n",
    "                             self.iteration / self.num_training_steps)**0.9\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr_\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metrics_dict = metrics.classification_report(predictions.cpu().detach().numpy(),batch['labels'].cpu().detach().numpy(),digits = 4,output_dict=True)\n",
    "        self.writer.add_scalar('info/loss',loss,self.iteration)\n",
    "        self.writer.add_scalar('info/weighted_avg',metrics_dict['weighted avg']['f1-score'],self.iteration)\n",
    "        self.iteration += 1\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metrics_dict = metrics.classification_report(predictions.cpu().detach().numpy(),batch['labels'].cpu().detach().numpy(),digits = 4,output_dict=True)\n",
    "        self.log('macro_avg',metrics_dict['macro avg']['f1-score'])\n",
    "        #self.metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        return {'val_loss':outputs.loss,'predictions':predictions,'labels':batch['labels']}\n",
    "    \n",
    "       \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        return {'test_loss':outputs.loss,'predictions':predictions}\n",
    "            \n",
    "    def training_epoch_end(self,outputs):\n",
    "        pass\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print(outputs[0]['predictions'].shape)\n",
    "        print(len(outputs))\n",
    "        val_loss ,predictions,labels= self.metrics_compute('val',outputs)\n",
    "        print(predictions.shape)\n",
    "        print('\\n',\"val_loss: \",val_loss.mean())\n",
    "        print(metrics.classification_report(predictions, labels,digits = 4))\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        predictions = outputs[0]['predictions']\n",
    "        for i in range(1,len(outputs)):\n",
    "            predictions = torch.concat([predictions,outputs[i]['predictions']],dim=0)\n",
    "        predictions = predictions.cpu().detach().numpy().tolist()\n",
    "        test_labels =[ idx2label[idx] for idx in predictions]\n",
    "        testdata_temp['label'] = test_labels\n",
    "        test_pred_list = []\n",
    "        for i in range(testdata_temp.shape[0]):\n",
    "            temp_dict = {}\n",
    "            temp_dict['id'] = testdata_temp.iloc[i,0]\n",
    "            temp_dict['label'] = testdata_temp.iloc[i,2]\n",
    "            temp_dict['text'] = testdata_temp.iloc[i,1]\n",
    "            test_pred_list.append(temp_dict)\n",
    "        print('\\n',testdata_temp.head())\n",
    "        with open(\"result.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(test_pred_list, fp, ensure_ascii=False, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "output_baseline: 0.7829 0.8543 0.8060 3 \n",
    "output_baseline + process: 0.7718 0.8712 0.8092 3\n",
    "output_baseline + process + aug: 0.7892 0.8523 0.8136 3\n",
    "output_baseline + process + TAPT: 0.8135 0.8525 0.8274 5\n",
    "output_baseline + process + + aug + TAPT: 0.7866 0.8773 0.8230\n",
    "output_baseline + process + + aug + TAPT + 10layer:0.7918 0.8714 0.8254 \n",
    "'''\n",
    "print(testdata.shape)\n",
    "print(testdata_temp.shape)\n",
    "\n",
    "model = Bert4textclassification_lightningsystem.load_from_checkpoint(checkpoint_path='./output_baseline/baseline+process+aug+TAPT/hflchinese-bert-wwm-ext-CHIP-CTC-epoch=02-macro_avg=0.7814.ckpt',\n",
    "net=net,lr=lr,epoch=num_epochs,len=num_training_steps\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "            logger=False,\n",
    "            gpus=1,\n",
    "            #limit_train_batches=0.05\n",
    "            #precision=16,\n",
    "            #accumulate_grad_batches=2,\n",
    "            #gradient_clip_val=0.5,\n",
    "        )\n",
    "trainer.test(model=model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90b94d728fc700a31971639fb0a12048c9a464edc079be00035098c1bbe95ef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
