{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22962\n",
      "7682\n",
      "10192\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'label', 'text', 'labels'],\n",
      "        num_rows: 22962\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'label', 'text', 'labels'],\n",
      "        num_rows: 7682\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 10192\n",
      "    })\n",
      "})\n",
      "{'id': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None)}\n",
      "{'id': 's1', 'label': 'Therapy or Surgery', 'text': ' 研究开始前30天内，接受过其他临床方案治疗；', 'labels': 11}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "\n",
    "\n",
    "traindata = pd.read_json('./CHIP-CTC/CHIP-CTC_train.json')\n",
    "valdata = pd.read_json('./CHIP-CTC/CHIP-CTC_dev.json')\n",
    "testdata = pd.read_json('./CHIP-CTC/CHIP-CTC_test.json')\n",
    "\n",
    "examplepreddata = pd.read_excel('./CHIP-CTC/category.xlsx')\n",
    "\n",
    "examplepreddata['label2idx'] = range(examplepreddata.shape[0])\n",
    "\n",
    "label2idx = dict(\n",
    "    zip(examplepreddata['Label Name'], examplepreddata['label2idx']))\n",
    "\n",
    "traindata['labels'] = [label2idx[item] for item in traindata['label']]\n",
    "valdata['labels'] = [label2idx[item] for item in valdata['label']]\n",
    "\n",
    "print(len(traindata))\n",
    "print(len(valdata))\n",
    "print(len(testdata))\n",
    "\n",
    "traindataset = Dataset.from_pandas(traindata)\n",
    "valdataset = Dataset.from_pandas(valdata)\n",
    "testdataset = Dataset.from_pandas(testdata)\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': traindataset,\n",
    "    'validation': valdataset,\n",
    "    'test': testdataset\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "print(train_dataset.features)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:03<00:00,  7.59ba/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  7.87ba/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  8.47ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 22962\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 7682\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 10192\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('uer/chinese_roberta_L-8_H-512',\n",
    "                                          mirror='tuna')\n",
    "\n",
    "def tokenize_function(sample):\n",
    "    return tokenizer(sample['text'], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(\n",
    "    ['id', 'text', 'label'])\n",
    "tokenized_datasets['validation'] = tokenized_datasets[\n",
    "    'validation'].remove_columns(['id', 'text', 'label'])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns(\n",
    "    ['id', 'text'])\n",
    "\n",
    "from transformers import DataCollatorWithPadding  #实现按batch自动padding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([8, 36]), 'token_type_ids': torch.Size([8, 36]), 'attention_mask': torch.Size([8, 36])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=8, collate_fn=data_collator)  \n",
    "val_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=8, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=8, collate_fn=data_collator)\n",
    "for batch in test_dataloader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.0945, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3595, -0.8232, -0.5250, -0.1728, -0.6569, -0.2318, -0.5023, -1.2957,\n",
      "         -0.6132, -1.1516, -0.4859, -0.2183, -1.7783, -1.8908, -1.2756, -1.4352,\n",
      "         -0.2314, -1.0946, -0.5662, -0.6864, -0.2688, -1.2383, -2.1606, -1.1244,\n",
      "         -2.4606,  0.5503, -1.1100, -0.3110,  1.5770, -1.9208,  4.5506, -0.4792,\n",
      "         -2.2947, -1.9314, -0.5908, -1.6749, -1.1083, -1.6656, -1.4088, -0.8657,\n",
      "         -1.0437, -1.8180, -0.9556,  5.3148],\n",
      "        [-0.3459, -0.5569, -0.3527,  8.1914, -0.0153,  0.3508, -0.5420, -0.8616,\n",
      "          0.0102,  0.4932,  0.4420,  0.3072, -0.6349, -0.8259, -0.2744,  0.2811,\n",
      "         -0.7900, -0.2973,  0.7945, -1.1079, -1.1131, -0.1070,  0.0590, -0.6125,\n",
      "         -0.4432, -0.9526, -0.0090, -0.5777, -0.9088,  0.2086,  0.3597, -1.7714,\n",
      "         -0.1879, -0.6060, -0.0837, -0.4378,  0.3161,  0.2662,  0.5132, -0.3632,\n",
      "         -0.0821,  0.2848, -0.3562,  0.6461],\n",
      "        [-0.3828, -0.7881, -0.9279,  0.2412, -1.3267, -1.6731, -0.6571, -0.0090,\n",
      "         -0.7000,  0.0094, -0.2146,  0.1773, -0.2451, -0.6935,  0.2505, -0.2462,\n",
      "         -0.4145, -0.3589,  8.8479, -0.0705,  0.4006, -1.3244, -0.0329, -0.1110,\n",
      "         -0.1128,  0.5520,  0.8243,  0.2680, -0.0492, -0.6382,  0.4980, -0.3866,\n",
      "         -0.3228, -0.1429, -0.0930,  0.1895, -0.6456, -0.1116, -0.9890,  0.2604,\n",
      "         -0.8711, -0.0763, -0.6396,  0.0288],\n",
      "        [ 0.6093, -0.5111, -0.3027,  0.1527, -0.6478, -1.1236,  1.4462, -0.8448,\n",
      "         -1.3947, -0.6597, -0.0723,  0.4659, -1.1928, -2.3751,  1.2568,  0.2908,\n",
      "         -0.2822, -1.4122, -0.6865, -2.1673, -1.4167, -1.2621, -2.0217, -1.7664,\n",
      "         -2.2870,  0.1589, -1.4061, -0.5082, -0.9593, -2.5266, -0.3860, -0.7084,\n",
      "         -2.6785, -2.3625, -0.8292, -1.4895, -1.4872, -2.2977, -1.4815, -1.4450,\n",
      "         -1.4920, -1.6461, -1.2117,  7.7114],\n",
      "        [ 1.9760, -0.6370,  0.0722, -0.8408, -0.7773, -1.2921, -0.7465,  1.5581,\n",
      "         -1.7784, -1.0703, -0.2746,  0.2977, -1.6061, -2.8504,  0.4285,  1.0203,\n",
      "          0.8090, -1.6960, -0.7540, -2.0515, -1.0915, -1.8600, -2.4780, -2.3237,\n",
      "         -2.8731,  0.1489, -1.7952, -0.6881, -0.7156, -2.9864, -0.6280, -0.5990,\n",
      "         -2.5767, -2.1875, -1.2865, -1.7751, -1.6490, -2.6636, -1.8339, -1.9741,\n",
      "         -1.7632, -1.6414, -1.7772,  8.2574],\n",
      "        [ 0.2529, -0.9592,  0.4004, -0.6086, -0.8456, -1.2187, -0.1819,  0.7439,\n",
      "         -1.6180, -1.0286,  0.0121,  0.1261, -1.0468, -2.4348,  0.0701,  4.7018,\n",
      "          1.1987, -1.4872, -0.7957, -2.4016, -1.4649, -1.7720, -2.3670, -2.3114,\n",
      "         -2.2579, -0.2589, -0.9691, -0.0472, -1.7044, -2.8123, -1.0024, -0.4356,\n",
      "         -2.2914, -1.6494, -0.7107, -1.2332, -1.5355, -1.9731, -1.4548, -2.1366,\n",
      "         -1.9235, -1.5397, -1.3945,  6.9145],\n",
      "        [-0.3493, -0.4921, -0.3965, -1.2266, -2.0195, -1.4690,  0.0260, -0.8965,\n",
      "         -0.7919, -1.3294, -0.8446,  0.8282, -1.0440, -1.4072,  0.2111,  0.4826,\n",
      "         -0.1770, -0.6066, -0.3032, -0.5113, -0.8787, -2.0106, -1.1330, -1.1517,\n",
      "         -1.3211,  1.1979, -0.0723,  6.9616, -0.0400, -1.1084,  0.5768,  0.2451,\n",
      "         -1.2753, -0.9557,  0.3656, -0.4528, -1.1694, -1.1244, -1.0282, -0.9886,\n",
      "         -0.9992, -0.7485, -1.3327,  3.4296],\n",
      "        [ 4.6473, -0.5062,  0.1826, -0.3805, -0.2212, -0.7555, -0.0930, -0.1747,\n",
      "         -1.9385, -1.3028,  0.1242,  0.2273, -1.7027, -3.1345,  0.5445,  0.0608,\n",
      "         -0.1929, -1.9180, -1.2470, -2.0423, -1.5830, -2.2429, -2.8473, -2.6267,\n",
      "         -3.2112,  0.2632, -2.4901, -0.9781, -0.8094, -3.0797, -0.3052, -0.4561,\n",
      "         -2.7262, -2.6624, -1.4401, -2.0497, -1.8669, -2.4860, -2.3462, -1.9318,\n",
      "         -1.7796, -1.9985, -2.1981,  7.6986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = './results/checkpoint-7000'\n",
    "net = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=examplepreddata.shape[0])\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    outputs = net(**batch)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type                          | Params\n",
      "-------------------------------------------------------\n",
      "0 | net  | BertForSequenceClassification | 36.6 M\n",
      "-------------------------------------------------------\n",
      "36.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "36.6 M    Total params\n",
      "146.344   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3832 [00:00<?, ?it/s]tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 1/3832 [00:00<05:45, 11.08it/s, loss=0.728]tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 2/3832 [00:00<04:20, 14.70it/s, loss=0.392]tensor(0.7714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 3/3832 [00:00<03:50, 16.60it/s, loss=0.518]tensor(0.5484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 4/3832 [00:00<03:37, 17.63it/s, loss=0.526]tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 5/3832 [00:00<03:27, 18.44it/s, loss=0.498]tensor(0.3662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 6/3832 [00:00<03:20, 19.12it/s, loss=0.476]tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 7/3832 [00:00<03:14, 19.71it/s, loss=0.42] tensor(1.2802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 8/3832 [00:00<03:11, 19.94it/s, loss=0.527]tensor(0.2891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 9/3832 [00:00<03:11, 19.94it/s, loss=0.501]tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 10/3832 [00:00<03:07, 20.37it/s, loss=0.484]tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 11/3832 [00:00<03:03, 20.86it/s, loss=0.501]tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 12/3832 [00:00<02:59, 21.25it/s, loss=0.472]tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 13/3832 [00:00<02:56, 21.63it/s, loss=0.463]tensor(0.3379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 14/3832 [00:00<02:53, 22.02it/s, loss=0.454]tensor(0.3677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 15/3832 [00:00<02:51, 22.27it/s, loss=0.448]tensor(0.9319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 16/3832 [00:00<02:50, 22.33it/s, loss=0.479]tensor(0.5030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 17/3832 [00:00<02:54, 21.87it/s, loss=0.48] tensor(0.8752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 18/3832 [00:00<02:54, 21.85it/s, loss=0.502]tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   0%|          | 19/3832 [00:00<02:54, 21.81it/s, loss=0.491]tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 20/3832 [00:00<02:54, 21.85it/s, loss=0.471]tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 21/3832 [00:00<02:54, 21.83it/s, loss=0.439]tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 22/3832 [00:00<02:52, 22.03it/s, loss=0.452]tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 23/3832 [00:01<02:51, 22.19it/s, loss=0.438]tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 24/3832 [00:01<02:50, 22.38it/s, loss=0.413]tensor(0.8402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 25/3832 [00:01<02:50, 22.39it/s, loss=0.436]tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 26/3832 [00:01<02:50, 22.35it/s, loss=0.422]tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 27/3832 [00:01<02:50, 22.26it/s, loss=0.427]tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 28/3832 [00:01<02:49, 22.39it/s, loss=0.382]tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 29/3832 [00:01<02:48, 22.55it/s, loss=0.373]tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 30/3832 [00:01<02:48, 22.58it/s, loss=0.395]tensor(0.7178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 31/3832 [00:01<02:49, 22.44it/s, loss=0.397]tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 32/3832 [00:01<02:49, 22.41it/s, loss=0.401]tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 33/3832 [00:01<02:48, 22.54it/s, loss=0.392]tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 34/3832 [00:01<02:49, 22.41it/s, loss=0.399]tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 35/3832 [00:01<02:49, 22.41it/s, loss=0.384]tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 36/3832 [00:01<02:48, 22.57it/s, loss=0.346]tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 37/3832 [00:01<02:47, 22.70it/s, loss=0.324]tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 38/3832 [00:01<02:46, 22.83it/s, loss=0.292]tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 39/3832 [00:01<02:46, 22.75it/s, loss=0.311]tensor(0.4914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 40/3832 [00:01<02:45, 22.86it/s, loss=0.331]tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 41/3832 [00:01<02:46, 22.81it/s, loss=0.328]tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 42/3832 [00:01<02:46, 22.74it/s, loss=0.326]tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 43/3832 [00:01<02:46, 22.78it/s, loss=0.319]tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 44/3832 [00:01<02:46, 22.77it/s, loss=0.319]tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 45/3832 [00:01<02:46, 22.72it/s, loss=0.283]tensor(0.3009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 46/3832 [00:02<02:46, 22.69it/s, loss=0.294]tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|          | 47/3832 [00:02<02:46, 22.78it/s, loss=0.304]tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 48/3832 [00:02<02:47, 22.61it/s, loss=0.286]tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 49/3832 [00:02<02:47, 22.60it/s, loss=0.301]tensor(0.9044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 50/3832 [00:02<02:47, 22.56it/s, loss=0.307]tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 51/3832 [00:02<02:47, 22.58it/s, loss=0.279]tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 52/3832 [00:02<02:47, 22.57it/s, loss=0.278]tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 53/3832 [00:02<02:47, 22.62it/s, loss=0.273]tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 54/3832 [00:02<02:47, 22.58it/s, loss=0.268]tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 55/3832 [00:02<02:47, 22.61it/s, loss=0.284]tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 56/3832 [00:02<02:47, 22.50it/s, loss=0.279]tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   1%|▏         | 57/3832 [00:02<02:47, 22.53it/s, loss=0.296]tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 58/3832 [00:02<02:47, 22.58it/s, loss=0.289]tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 59/3832 [00:02<02:46, 22.63it/s, loss=0.282]tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 60/3832 [00:02<02:46, 22.65it/s, loss=0.276]tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 61/3832 [00:02<02:46, 22.71it/s, loss=0.314]tensor(0.3185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 62/3832 [00:02<02:45, 22.75it/s, loss=0.316]tensor(0.7284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 63/3832 [00:02<02:45, 22.72it/s, loss=0.335]tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 64/3832 [00:02<02:46, 22.67it/s, loss=0.351]tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 65/3832 [00:02<02:45, 22.69it/s, loss=0.349]tensor(0.9102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 66/3832 [00:02<02:45, 22.73it/s, loss=0.38] tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 67/3832 [00:02<02:45, 22.77it/s, loss=0.366]tensor(0.8454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 68/3832 [00:02<02:45, 22.79it/s, loss=0.407]tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 69/3832 [00:03<02:44, 22.85it/s, loss=0.391]tensor(0.7400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 70/3832 [00:03<02:44, 22.89it/s, loss=0.383]tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 71/3832 [00:03<02:44, 22.79it/s, loss=0.376]tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 72/3832 [00:03<02:45, 22.78it/s, loss=0.372]tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 73/3832 [00:03<02:44, 22.80it/s, loss=0.375]tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 74/3832 [00:03<02:44, 22.84it/s, loss=0.358]tensor(0.4359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 75/3832 [00:03<02:44, 22.88it/s, loss=0.36] tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 76/3832 [00:03<02:43, 22.92it/s, loss=0.382]tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 77/3832 [00:03<02:43, 22.94it/s, loss=0.399]tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 78/3832 [00:03<02:43, 22.97it/s, loss=0.411]tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 79/3832 [00:03<02:43, 22.91it/s, loss=0.421]tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 80/3832 [00:03<02:43, 22.95it/s, loss=0.408]tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 81/3832 [00:03<02:43, 23.01it/s, loss=0.38] tensor(0.2870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 82/3832 [00:03<02:43, 22.97it/s, loss=0.378]tensor(1.1619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 83/3832 [00:03<02:42, 23.00it/s, loss=0.4]  tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 84/3832 [00:03<02:42, 23.05it/s, loss=0.41]tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 85/3832 [00:03<02:42, 23.07it/s, loss=0.425]tensor(0.5658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 86/3832 [00:03<02:42, 23.08it/s, loss=0.407]tensor(1.2654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 87/3832 [00:03<02:42, 23.07it/s, loss=0.466]tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 88/3832 [00:03<02:42, 23.09it/s, loss=0.438]tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 89/3832 [00:03<02:41, 23.13it/s, loss=0.463]tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 90/3832 [00:03<02:41, 23.18it/s, loss=0.438]tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 91/3832 [00:03<02:41, 23.23it/s, loss=0.448]tensor(0.4124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 92/3832 [00:03<02:41, 23.19it/s, loss=0.463]tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 93/3832 [00:04<02:40, 23.24it/s, loss=0.465]tensor(1.1064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 94/3832 [00:04<02:40, 23.26it/s, loss=0.519]tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   2%|▏         | 95/3832 [00:04<02:40, 23.26it/s, loss=0.498]tensor(1.3448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 96/3832 [00:04<02:40, 23.25it/s, loss=0.539]tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 97/3832 [00:04<02:40, 23.27it/s, loss=0.505]tensor(0.3432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 98/3832 [00:04<02:40, 23.26it/s, loss=0.504]tensor(0.4538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 99/3832 [00:04<02:40, 23.25it/s, loss=0.491]tensor(0.5387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 100/3832 [00:04<02:40, 23.25it/s, loss=0.512]tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 101/3832 [00:04<02:40, 23.20it/s, loss=0.516]tensor(0.4445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 102/3832 [00:04<02:40, 23.21it/s, loss=0.524]tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 103/3832 [00:04<02:40, 23.24it/s, loss=0.497]tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 104/3832 [00:04<02:40, 23.22it/s, loss=0.473]tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 105/3832 [00:04<02:40, 23.22it/s, loss=0.491]tensor(1.2752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 106/3832 [00:04<02:40, 23.24it/s, loss=0.526]tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 107/3832 [00:04<02:40, 23.22it/s, loss=0.489]tensor(0.4102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 108/3832 [00:04<02:40, 23.24it/s, loss=0.495]tensor(0.4958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 109/3832 [00:04<02:40, 23.15it/s, loss=0.491]tensor(0.7915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 110/3832 [00:04<02:41, 23.07it/s, loss=0.518]tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 111/3832 [00:04<02:41, 23.07it/s, loss=0.541]tensor(0.8724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 112/3832 [00:04<02:41, 23.07it/s, loss=0.564]tensor(0.5611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 113/3832 [00:04<02:41, 23.09it/s, loss=0.584]tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 114/3832 [00:04<02:40, 23.10it/s, loss=0.561]tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 115/3832 [00:04<02:41, 23.07it/s, loss=0.573]tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 116/3832 [00:05<02:40, 23.11it/s, loss=0.535]tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 117/3832 [00:05<02:40, 23.08it/s, loss=0.544]tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 118/3832 [00:05<02:40, 23.12it/s, loss=0.563]tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 119/3832 [00:05<02:40, 23.12it/s, loss=0.547]tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 120/3832 [00:05<02:40, 23.13it/s, loss=0.524]tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 121/3832 [00:05<02:40, 23.14it/s, loss=0.516]tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 122/3832 [00:05<02:40, 23.17it/s, loss=0.496]tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 123/3832 [00:05<02:40, 23.16it/s, loss=0.481]tensor(0.4628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 124/3832 [00:05<02:40, 23.16it/s, loss=0.498]tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 125/3832 [00:05<02:39, 23.18it/s, loss=0.468]tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 126/3832 [00:05<02:39, 23.18it/s, loss=0.407]tensor(0.9311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 127/3832 [00:05<02:39, 23.17it/s, loss=0.427]tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 128/3832 [00:05<02:40, 23.14it/s, loss=0.408]tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 129/3832 [00:05<02:40, 23.07it/s, loss=0.398]tensor(0.4776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 130/3832 [00:05<02:40, 23.10it/s, loss=0.382]tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 131/3832 [00:05<02:40, 23.13it/s, loss=0.385]tensor(1.8873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 132/3832 [00:05<02:40, 23.11it/s, loss=0.436]tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 133/3832 [00:05<02:40, 23.10it/s, loss=0.439]tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   3%|▎         | 134/3832 [00:05<02:40, 23.09it/s, loss=0.437]tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 135/3832 [00:05<02:40, 23.08it/s, loss=0.429]tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 136/3832 [00:05<02:40, 23.08it/s, loss=0.41] tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 137/3832 [00:05<02:40, 23.03it/s, loss=0.409]tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 138/3832 [00:05<02:40, 23.02it/s, loss=0.401]tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 139/3832 [00:06<02:40, 23.04it/s, loss=0.399]tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 140/3832 [00:06<02:40, 23.04it/s, loss=0.421]tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 141/3832 [00:06<02:40, 23.06it/s, loss=0.431]tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 142/3832 [00:06<02:39, 23.09it/s, loss=0.457]tensor(0.4471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▎         | 143/3832 [00:06<02:40, 23.05it/s, loss=0.464]tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 144/3832 [00:06<02:39, 23.07it/s, loss=0.445]tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 145/3832 [00:06<02:39, 23.08it/s, loss=0.452]tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 146/3832 [00:06<02:39, 23.09it/s, loss=0.459]tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 147/3832 [00:06<02:39, 23.11it/s, loss=0.445]tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 148/3832 [00:06<02:39, 23.13it/s, loss=0.461]tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 149/3832 [00:06<02:39, 23.16it/s, loss=0.459]tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 150/3832 [00:06<02:39, 23.13it/s, loss=0.468]tensor(0.4254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 151/3832 [00:06<02:38, 23.15it/s, loss=0.451]tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 152/3832 [00:06<02:38, 23.15it/s, loss=0.361]tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 153/3832 [00:06<02:39, 23.13it/s, loss=0.334]tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 154/3832 [00:06<02:39, 23.10it/s, loss=0.321]tensor(0.4582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 155/3832 [00:06<02:39, 23.11it/s, loss=0.339]tensor(1.2960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 156/3832 [00:06<02:39, 23.09it/s, loss=0.393]tensor(0.9241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 157/3832 [00:06<02:39, 23.08it/s, loss=0.43] tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 158/3832 [00:06<02:39, 23.08it/s, loss=0.424]tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 159/3832 [00:06<02:39, 23.08it/s, loss=0.44] tensor(0.3346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 160/3832 [00:06<02:39, 23.07it/s, loss=0.431]tensor(0.9727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 161/3832 [00:06<02:39, 23.08it/s, loss=0.462]tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 162/3832 [00:07<02:38, 23.08it/s, loss=0.438]tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 163/3832 [00:07<02:39, 23.05it/s, loss=0.445]tensor(1.3635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 164/3832 [00:07<02:39, 23.04it/s, loss=0.508]tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 165/3832 [00:07<02:39, 23.05it/s, loss=0.498]tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 166/3832 [00:07<02:38, 23.06it/s, loss=0.518]tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 167/3832 [00:07<02:38, 23.09it/s, loss=0.491]tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 168/3832 [00:07<02:38, 23.12it/s, loss=0.509]tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 169/3832 [00:07<02:38, 23.08it/s, loss=0.5]  tensor(1.1980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 170/3832 [00:07<02:38, 23.08it/s, loss=0.526]tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 171/3832 [00:07<02:38, 23.10it/s, loss=0.517]tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   4%|▍         | 172/3832 [00:07<02:38, 23.10it/s, loss=0.537]tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 173/3832 [00:07<02:38, 23.09it/s, loss=0.536]tensor(0.5164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 174/3832 [00:07<02:38, 23.11it/s, loss=0.544]tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 175/3832 [00:07<02:38, 23.14it/s, loss=0.532]tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 176/3832 [00:07<02:37, 23.15it/s, loss=0.472]tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 177/3832 [00:07<02:37, 23.16it/s, loss=0.439]tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 178/3832 [00:07<02:38, 23.12it/s, loss=0.446]tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 179/3832 [00:07<02:38, 23.11it/s, loss=0.441]tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 180/3832 [00:07<02:38, 23.10it/s, loss=0.431]tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 181/3832 [00:07<02:38, 23.09it/s, loss=0.395]tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 182/3832 [00:07<02:38, 23.09it/s, loss=0.402]tensor(0.4428, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 183/3832 [00:07<02:37, 23.10it/s, loss=0.395]tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 184/3832 [00:07<02:38, 23.07it/s, loss=0.33] tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 185/3832 [00:08<02:38, 23.07it/s, loss=0.333]tensor(0.4933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 186/3832 [00:08<02:37, 23.08it/s, loss=0.328]tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 187/3832 [00:08<02:37, 23.09it/s, loss=0.328]tensor(0.4380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 188/3832 [00:08<02:37, 23.09it/s, loss=0.315]tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 189/3832 [00:08<02:37, 23.10it/s, loss=0.348]tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 190/3832 [00:08<02:37, 23.08it/s, loss=0.291]tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▍         | 191/3832 [00:08<02:37, 23.07it/s, loss=0.31] tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 192/3832 [00:08<02:37, 23.08it/s, loss=0.296]tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 193/3832 [00:08<02:37, 23.10it/s, loss=0.298]tensor(0.2877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 194/3832 [00:08<02:37, 23.11it/s, loss=0.286]tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 195/3832 [00:08<02:37, 23.13it/s, loss=0.279]tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 196/3832 [00:08<02:37, 23.15it/s, loss=0.309]tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 197/3832 [00:08<02:37, 23.13it/s, loss=0.299]tensor(0.4417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 198/3832 [00:08<02:37, 23.12it/s, loss=0.292]tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 199/3832 [00:08<02:37, 23.14it/s, loss=0.311]tensor(0.9325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 200/3832 [00:08<02:36, 23.16it/s, loss=0.351]tensor(0.9063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 201/3832 [00:08<02:36, 23.18it/s, loss=0.384]tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 202/3832 [00:08<02:36, 23.20it/s, loss=0.402]tensor(0.7446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 203/3832 [00:08<02:36, 23.20it/s, loss=0.417]tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 204/3832 [00:08<02:36, 23.21it/s, loss=0.424]tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 205/3832 [00:08<02:36, 23.22it/s, loss=0.449]tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 206/3832 [00:08<02:36, 23.19it/s, loss=0.425]tensor(0.4050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 207/3832 [00:08<02:36, 23.19it/s, loss=0.44] tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 208/3832 [00:08<02:36, 23.18it/s, loss=0.433]tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 209/3832 [00:09<02:36, 23.19it/s, loss=0.421]tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   5%|▌         | 210/3832 [00:09<02:36, 23.20it/s, loss=0.421]tensor(1.1921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 211/3832 [00:09<02:35, 23.22it/s, loss=0.45] tensor(0.8630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 212/3832 [00:09<02:35, 23.24it/s, loss=0.482]tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 213/3832 [00:09<02:35, 23.25it/s, loss=0.485]tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 214/3832 [00:09<02:35, 23.24it/s, loss=0.472]tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 215/3832 [00:09<02:35, 23.25it/s, loss=0.481]tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 216/3832 [00:09<02:35, 23.27it/s, loss=0.482]tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 217/3832 [00:09<02:35, 23.28it/s, loss=0.486]tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 218/3832 [00:09<02:35, 23.28it/s, loss=0.488]tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 219/3832 [00:09<02:35, 23.28it/s, loss=0.475]tensor(1.2450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 220/3832 [00:09<02:35, 23.29it/s, loss=0.49] tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 221/3832 [00:09<02:35, 23.27it/s, loss=0.47]tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 222/3832 [00:09<02:35, 23.26it/s, loss=0.457]tensor(0.5745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 223/3832 [00:09<02:35, 23.26it/s, loss=0.449]tensor(1.5174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 224/3832 [00:09<02:35, 23.24it/s, loss=0.514]tensor(0.8868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 225/3832 [00:09<02:35, 23.24it/s, loss=0.527]tensor(0.5219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 226/3832 [00:09<02:35, 23.24it/s, loss=0.553]tensor(0.4204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 227/3832 [00:09<02:35, 23.22it/s, loss=0.553]tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 228/3832 [00:09<02:35, 23.23it/s, loss=0.554]tensor(0.3252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 229/3832 [00:09<02:35, 23.23it/s, loss=0.546]tensor(1.0752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 230/3832 [00:09<02:35, 23.23it/s, loss=0.598]tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 231/3832 [00:09<02:35, 23.23it/s, loss=0.54] tensor(0.8908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 232/3832 [00:09<02:34, 23.23it/s, loss=0.542]tensor(0.8013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 233/3832 [00:10<02:34, 23.24it/s, loss=0.575]tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 234/3832 [00:10<02:34, 23.22it/s, loss=0.606]tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 235/3832 [00:10<02:34, 23.23it/s, loss=0.604]tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 236/3832 [00:10<02:34, 23.23it/s, loss=0.595]tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 237/3832 [00:10<02:34, 23.25it/s, loss=0.59] tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 238/3832 [00:10<02:34, 23.26it/s, loss=0.572]tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▌         | 239/3832 [00:10<02:34, 23.28it/s, loss=0.586]tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 240/3832 [00:10<02:34, 23.26it/s, loss=0.53] tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 241/3832 [00:10<02:34, 23.25it/s, loss=0.542]tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 242/3832 [00:10<02:34, 23.26it/s, loss=0.535]tensor(0.3645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 243/3832 [00:10<02:34, 23.26it/s, loss=0.524]tensor(0.8571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 244/3832 [00:10<02:34, 23.27it/s, loss=0.491]tensor(0.4561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 245/3832 [00:10<02:34, 23.28it/s, loss=0.47] tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 246/3832 [00:10<02:34, 23.25it/s, loss=0.474]tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 247/3832 [00:10<02:34, 23.25it/s, loss=0.491]tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 248/3832 [00:10<02:34, 23.23it/s, loss=0.503]tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   6%|▋         | 249/3832 [00:10<02:34, 23.22it/s, loss=0.5]  tensor(0.8653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   7%|▋         | 250/3832 [00:10<02:34, 23.22it/s, loss=0.49]tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   7%|▋         | 251/3832 [00:10<02:34, 23.22it/s, loss=0.494]tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   7%|▋         | 252/3832 [00:10<02:34, 23.23it/s, loss=0.472]tensor(1.2862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   7%|▋         | 253/3832 [00:10<02:34, 23.22it/s, loss=0.496]"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import AdamW, get_scheduler\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Bert4textclassification_lightningsystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,net,lr,epoch):\n",
    "        super(Bert4textclassification_lightningsystem, self).__init__()\n",
    "        self.net = net.to(device)\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        #self.metric = load_metric(\"glue\", \"mrpc\",mirror=\"tuna\")\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        self.optimizer = AdamW(self.net.parameters(), lr=self.lr)\n",
    "        return self.optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        loss = self.net(**batch).loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        #self.metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        self.log('val_loss', outputs.loss)\n",
    "    \n",
    "    def train_epoch_end(self,outputs):\n",
    "        print(sum(outputs) / len(outputs))\n",
    "        \n",
    "    def val_epoch_end(self, outputs):\n",
    "        print('1')\n",
    "        #print(self.metric.compute())\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        return predictions\n",
    "\n",
    "num_epoches = 3\n",
    "lr = 3e-5\n",
    "model = Bert4textclassification_lightningsystem(net,lr,num_epoches)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_loss',\n",
    "            dirpath='./output',\n",
    "            filename=\n",
    "            'chinese_roberta_L-8_H-512-CHIP-CTC-{epoch:02d}-{val_loss:.3f}',\n",
    "            mode='min')\n",
    "trainer = Trainer(\n",
    "            logger=False,\n",
    "            max_epochs=num_epoches,\n",
    "            gpus=1,\n",
    "            reload_dataloaders_every_n_epochs=False,\n",
    "            num_sanity_val_steps=0,  # Skip Sanity Check\n",
    "            callbacks=[checkpoint_callback],\n",
    "            #precision=16,\n",
    "            #accumulate_grad_batches=2,\n",
    "            #gradient_clip_val=0.5,\n",
    "        )\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90b94d728fc700a31971639fb0a12048c9a464edc079be00035098c1bbe95ef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
