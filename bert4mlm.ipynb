{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Disease', 1: 'Symptom', 2: 'Sign', 3: 'Pregnancy-related Activity', 4: 'Neoplasm Status', 5: 'Non-Neoplasm Disease Stage', 6: 'Allergy Intolerance', 7: 'Organ or Tissue Status', 8: 'Life Expectancy', 9: 'Oral related', 10: 'Pharmaceutical Substance or Drug', 11: 'Therapy or Surgery', 12: 'Device', 13: 'Nursing', 14: 'Diagnostic', 15: 'Laboratory Examinations', 16: 'Risk Assessment', 17: 'Receptor Status', 18: 'Age', 19: 'Special Patient Characteristic', 20: 'Literacy', 21: 'Gender', 22: 'Education', 23: 'Address', 24: 'Ethnicity', 25: 'Consent', 26: 'Enrollment in other studies', 27: 'Researcher Decision', 28: 'Capacity', 29: 'Ethical Audit', 30: 'Compliance with Protocol', 31: 'Addictive Behavior', 32: 'Bedtime', 33: 'Exercise', 34: 'Diet', 35: 'Alcohol Consumer', 36: 'Sexual related', 37: 'Smoking Status', 38: 'Blood Donation', 39: 'Encounter', 40: 'Disabilities', 41: 'Healthy', 42: 'Data Accessible', 43: 'Multiple'}\n",
      "22962\n",
      "7682\n",
      "10192\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'label', 'text', 'labels'],\n",
      "        num_rows: 22962\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'label', 'text', 'labels'],\n",
      "        num_rows: 7682\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 10192\n",
      "    })\n",
      "})\n",
      "{'id': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None)}\n",
      "{'id': 's1', 'label': 'Therapy or Surgery', 'text': ' 研究开始前30天内，接受过其他临床方案治疗；', 'labels': 11}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seedeverything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  ##\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seedeverything(seed=233)\n",
    "\n",
    "traindata = pd.read_json('./CHIP-CTC/CHIP-CTC_train.json')\n",
    "valdata = pd.read_json('./CHIP-CTC/CHIP-CTC_dev.json')\n",
    "testdata = pd.read_json('./CHIP-CTC/CHIP-CTC_test.json')\n",
    "\n",
    "examplepreddata = pd.read_excel('./CHIP-CTC/category.xlsx')\n",
    "\n",
    "examplepreddata['label2idx'] = range(examplepreddata.shape[0])\n",
    "\n",
    "label2idx = dict(\n",
    "    zip(examplepreddata['Label Name'], examplepreddata['label2idx']))\n",
    "idx2label = dict(\n",
    "    zip(examplepreddata['label2idx'], examplepreddata['Label Name']))\n",
    "\n",
    "print(idx2label)\n",
    "\n",
    "traindata['labels'] = [label2idx[item] for item in traindata['label']]\n",
    "valdata['labels'] = [label2idx[item] for item in valdata['label']]\n",
    "\n",
    "print(len(traindata))\n",
    "print(len(valdata))\n",
    "print(len(testdata))\n",
    "\n",
    "\n",
    "traindataset = Dataset.from_pandas(traindata)\n",
    "valdataset = Dataset.from_pandas(valdata)\n",
    "testdataset = Dataset.from_pandas(testdata)\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': traindataset,\n",
    "    'validation': valdataset,\n",
    "    'test': testdataset\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "print(train_dataset.features)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "from ltp import LTP\n",
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "\n",
    "\n",
    "def _is_chinese_char(cp):\n",
    "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "    #\n",
    "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "    # space-separated words, so they are not treated specially and handled\n",
    "    # like the all of the other languages.\n",
    "    if (\n",
    "        (cp >= 0x4E00 and cp <= 0x9FFF)\n",
    "        or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
    "        or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
    "        or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
    "        or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
    "        or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
    "        or (cp >= 0xF900 and cp <= 0xFAFF)\n",
    "        or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
    "    ):  #\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_chinese(word: str):\n",
    "    # word like '180' or '身高' or '神'\n",
    "    for char in word:\n",
    "        char = ord(char)\n",
    "        if not _is_chinese_char(char):\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def get_chinese_word(tokens: List[str]):\n",
    "    word_set = set()\n",
    "\n",
    "    for token in tokens:\n",
    "        chinese_word = len(token) > 1 and is_chinese(token)\n",
    "        if chinese_word:\n",
    "            word_set.add(token)\n",
    "    word_list = list(word_set)\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def add_sub_symbol(bert_tokens: List[str], chinese_word_set: set()):\n",
    "    if not chinese_word_set:\n",
    "        return bert_tokens\n",
    "    max_word_len = max([len(w) for w in chinese_word_set])\n",
    "\n",
    "    bert_word = bert_tokens\n",
    "    start, end = 0, len(bert_word)\n",
    "    while start < end:\n",
    "        single_word = True\n",
    "        if is_chinese(bert_word[start]):\n",
    "            l = min(end - start, max_word_len)\n",
    "            for i in range(l, 1, -1):\n",
    "                whole_word = \"\".join(bert_word[start : start + i])\n",
    "                if whole_word in chinese_word_set:\n",
    "                    for j in range(start + 1, start + i):\n",
    "                        bert_word[j] = \"##\" + bert_word[j]\n",
    "                    start = start + i\n",
    "                    single_word = False\n",
    "                    break\n",
    "        if single_word:\n",
    "            start += 1\n",
    "    return bert_word\n",
    "\n",
    "\n",
    "def prepare_ref(lines: List[str], ltp_tokenizer: LTP, bert_tokenizer: BertTokenizer):\n",
    "    ltp_res = []\n",
    "\n",
    "    for i in range(0, len(lines), 100):\n",
    "        res = ltp_tokenizer.seg(lines[i : i + 100])[0]\n",
    "        res = [get_chinese_word(r) for r in res]\n",
    "        ltp_res.extend(res)\n",
    "    assert len(ltp_res) == len(lines)\n",
    "\n",
    "    bert_res = []\n",
    "    for i in range(0, len(lines), 100):\n",
    "        res = bert_tokenizer(lines[i : i + 100], add_special_tokens=True, truncation=True, max_length=512)\n",
    "        bert_res.extend(res[\"input_ids\"])\n",
    "    assert len(bert_res) == len(lines)\n",
    "\n",
    "    ref_ids = []\n",
    "    for input_ids, chinese_word in zip(bert_res, ltp_res):\n",
    "\n",
    "        input_tokens = []\n",
    "        for id in input_ids:\n",
    "            token = bert_tokenizer._convert_id_to_token(id)\n",
    "            input_tokens.append(token)\n",
    "        input_tokens = add_sub_symbol(input_tokens, chinese_word)\n",
    "        ref_id = []\n",
    "        # We only save pos of chinese subwords start with ##, which mean is part of a whole word.\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            if token[:2] == \"##\":\n",
    "                clean_token = token[2:]\n",
    "                # save chinese tokens' pos\n",
    "                if len(clean_token) == 1 and _is_chinese_char(ord(clean_token)):\n",
    "                    ref_id.append(i)\n",
    "        ref_ids.append(ref_id)\n",
    "\n",
    "    assert len(ref_ids) == len(bert_res)\n",
    "\n",
    "    return ref_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hflchinese-bert-wwm-ext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40836\n"
     ]
    }
   ],
   "source": [
    "from transformers.data.data_collator import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "path = \"hflchinese-bert-wwm-ext\"\n",
    "tokenizer = BertTokenizer.from_pretrained(path)\n",
    "model = BertForMaskedLM.from_pretrained(path)\n",
    "\n",
    "from ltp import LTP\n",
    "ltp = LTP()\n",
    "\n",
    "sent = [item['text'] for item in train_dataset] + [item['text'] for item in val_dataset] + [item['text'] for item in test_dataset]\n",
    "\n",
    "ref = prepare_ref(sent, ltp, tokenizer)\n",
    "print(len(ref))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 23/23 [00:03<00:00,  5.87ba/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  5.86ba/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  6.18ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hflchinese-bert-wwm-ext\")\n",
    "\n",
    "def tokenize_function(sample):\n",
    "    return tokenizer(sample['text'],truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(\n",
    "    ['id', 'text', 'label'])\n",
    "tokenized_datasets['validation'] = tokenized_datasets[\n",
    "    'validation'].remove_columns(['id', 'text', 'label'])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns(\n",
    "    ['id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dict = tokenized_datasets['train']['input_ids'] + tokenized_datasets['validation']['input_ids'] + tokenized_datasets['test']['input_ids']\n",
    "\n",
    "# 加上子字信息，而且传入的是List，不是tensor。\n",
    "train_mlm_dataset = [{'input_ids':encoder_dict[i],'chinese_ref':ref[i]} for i in range(len(ref))]\n",
    "\n",
    "datacollecter = DataCollatorForWholeWordMask(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([12, 101]), 'labels': torch.Size([12, 101])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding  #实现按batch自动padding\n",
    "\n",
    "train_mlm_dataloader = DataLoader(train_mlm_dataset, shuffle=True, batch_size=12, collate_fn=datacollecter)  \n",
    "for batch in train_mlm_dataloader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=tensor(4.5038, grad_fn=<NllLossBackward0>), logits=tensor([[[ -9.6789,  -8.2672,  -8.9909,  ...,  -9.1919, -10.4901,  -9.9213],\n",
      "         [-13.6715, -11.8114, -11.4480,  ..., -11.4196, -14.3877, -11.0136],\n",
      "         [-12.7726, -12.8846, -12.5715,  ..., -11.4791, -14.2202, -15.3056],\n",
      "         ...,\n",
      "         [-10.0236,  -6.6894,  -7.7634,  ..., -10.6219, -11.8329,  -6.0387],\n",
      "         [ -9.9980,  -6.7233,  -7.7489,  ..., -10.6563, -11.5828,  -5.7179],\n",
      "         [ -9.7295,  -6.7921,  -7.6953,  ..., -10.2023, -10.8552,  -5.5733]],\n",
      "\n",
      "        [[ -9.5558,  -8.1402,  -9.4869,  ...,  -8.2798,  -9.7860,  -9.6576],\n",
      "         [ -9.2906,  -8.3148,  -8.3864,  ...,  -7.7399,  -7.8785,  -7.2531],\n",
      "         [-11.2227, -12.7251, -11.8909,  ...,  -6.8628,  -8.1228, -13.7081],\n",
      "         ...,\n",
      "         [ -6.8631,  -5.3354,  -7.1390,  ...,  -7.8046,  -6.5177,  -2.9470],\n",
      "         [ -6.7111,  -5.3284,  -7.2786,  ...,  -7.6783,  -6.0707,  -2.3571],\n",
      "         [ -6.7783,  -5.5183,  -7.3706,  ...,  -7.3561,  -6.0869,  -2.6138]],\n",
      "\n",
      "        [[ -9.3929,  -8.1569,  -8.9894,  ...,  -8.9133,  -9.9274,  -9.3122],\n",
      "         [-12.5358, -13.0054, -11.7510,  ..., -13.6562, -15.1218, -10.7407],\n",
      "         [-13.8508, -12.3776, -10.2267,  ..., -14.6216, -15.5619, -15.5286],\n",
      "         ...,\n",
      "         [-10.4639,  -7.4467,  -8.2653,  ..., -11.3317, -11.5859,  -6.7013],\n",
      "         [-10.5385,  -7.5449,  -8.2403,  ..., -11.3938, -11.3735,  -6.5175],\n",
      "         [-10.2057,  -7.4335,  -8.0617,  ..., -10.7545, -10.6162,  -6.2311]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.0886,  -7.6834,  -8.7592,  ...,  -8.6991, -10.4762, -10.0315],\n",
      "         [ -7.6777,  -7.4077,  -7.6079,  ...,  -9.0458,  -8.8140,  -6.4611],\n",
      "         [-12.0690, -12.4511, -10.4028,  ..., -12.4495, -12.0120, -10.0713],\n",
      "         ...,\n",
      "         [ -6.8765,  -4.9247,  -5.9216,  ...,  -7.8143,  -8.3441,  -4.2391],\n",
      "         [ -6.7224,  -4.9245,  -5.9198,  ...,  -7.6679,  -7.9707,  -3.8711],\n",
      "         [ -6.5288,  -4.9748,  -5.8259,  ...,  -7.3805,  -7.6267,  -3.8558]],\n",
      "\n",
      "        [[ -9.8671,  -8.2966,  -9.3921,  ...,  -8.9720, -10.1715,  -8.6426],\n",
      "         [-11.8270, -10.4641, -10.7112,  ..., -11.0780, -14.2949,  -9.3680],\n",
      "         [-13.9496, -13.0871, -13.1868,  ..., -12.4927, -15.8763, -11.4311],\n",
      "         ...,\n",
      "         [-11.7908,  -8.8317,  -8.9564,  ..., -12.3614, -12.6592,  -7.0736],\n",
      "         [-11.8717,  -8.8767,  -8.7877,  ..., -12.3103, -12.3690,  -7.0003],\n",
      "         [-11.3772,  -8.7112,  -8.4993,  ..., -11.6527, -11.4021,  -6.6031]],\n",
      "\n",
      "        [[ -8.9458,  -7.7463,  -8.9394,  ...,  -6.7705,  -7.3432,  -6.4419],\n",
      "         [ -7.6410,  -7.6534,  -8.4715,  ...,  -8.5062,  -9.4267,  -5.5427],\n",
      "         [-10.9842,  -9.9515,  -7.8929,  ..., -10.1868, -10.5272,  -9.3474],\n",
      "         ...,\n",
      "         [ -7.1174,  -5.0205,  -6.9492,  ...,  -7.3794,  -8.4393,  -3.1868],\n",
      "         [ -6.9724,  -5.0315,  -7.0353,  ...,  -7.2048,  -8.0018,  -2.7248],\n",
      "         [ -6.9703,  -5.2003,  -6.9573,  ...,  -6.9935,  -7.9157,  -2.8823]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_mlm_dataloader:\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1271/3234628415.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamplepreddata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mnum_training_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# num of batches * num of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import AdamW, get_scheduler\n",
    "from datasets import load_metric\n",
    "from statistics import mean\n",
    "from sklearn import metrics\n",
    "from torch import nn\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 8\n",
    "lr = 3e-5\n",
    "num_labels = examplepreddata.shape[0] \n",
    "num_training_steps = num_epochs * len(train_mlm_dataloader)  # num of batches * num of epochs\n",
    "print(num_training_steps)\n",
    "\n",
    "class Bert4wwmtask_lightningsystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,net,lr,epoch,len):\n",
    "        super(Bert4wwmtask_lightningsystem, self).__init__()\n",
    "        self.net = net.to(device)\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.num_training_steps = len\n",
    "        #self.metric = load_metric(\"glue\", \"mrpc\",mirror=\"tuna\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        self.optimizer = AdamW(self.net.parameters(), lr=self.lr)\n",
    "        lr_scheduler = get_scheduler(\n",
    "                'linear',\n",
    "                optimizer=self.optimizer, \n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=self.num_training_steps)\n",
    "        optim_dict = {'optimizer': self.optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        return optim_dict\n",
    "        \n",
    "    def metrics_compute(self,mode,outputs):\n",
    "        loss = []\n",
    "        loss.append(outputs[0][mode+'_loss'])\n",
    "        predictions = outputs[0]['predictions']\n",
    "        labels = outputs[0]['labels']\n",
    "        for i in range(1,len(outputs)):\n",
    "            loss.append(outputs[i][mode+'_loss'])\n",
    "            predictions = torch.concat([predictions,outputs[i]['predictions']],dim=0)\n",
    "            labels = torch.concat([labels,outputs[i]['labels']],dim=0)\n",
    "        loss = torch.tensor(loss)\n",
    "        predictions = predictions.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        return loss,predictions,labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        loss = self.net(**batch).loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metrics_dict = metrics.classification_report(predictions.cpu().detach().numpy(),batch['labels'].cpu().detach().numpy(),digits = 4,output_dict=True)\n",
    "        self.log('val_f1',(metrics_dict['macro avg']['f1-score']+metrics_dict['weighted avg']['f1-score']+metrics_dict['accuracy'])/3.0,on_epoch=True, prog_bar=True, logger=True)\n",
    "        #self.metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        return {'val_loss':outputs.loss,'predictions':predictions,'labels':batch['labels']}\n",
    "    \n",
    "       \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = self.net(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        return {'test_loss':outputs.loss,'predictions':predictions}\n",
    "            \n",
    "    def training_epoch_end(self,outputs):\n",
    "        pass\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print(outputs[0]['predictions'].shape)\n",
    "        print(len(outputs))\n",
    "        val_loss ,predictions,labels= self.metrics_compute('val',outputs)\n",
    "        print(predictions.shape)\n",
    "        print('\\n',\"val_loss: \",val_loss.mean())\n",
    "        print(metrics.classification_report(predictions, labels,digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90b94d728fc700a31971639fb0a12048c9a464edc079be00035098c1bbe95ef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
